{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arlington 2050 Summary\n",
    "\n",
    "## Introduction -\n",
    "The Arlington 2050 project is a year long initiative that strives to get community feedback on what Arlington should look like in 2050 and the challenges we must address to get there. Rachael and I worked specifically with the 'Postcard test tracker dataset'. This specific dataset had data collected from many locations. These ranged from local libraries to community events. Our data was collected strictly on postcards at said locations, whether that be in person or dropped in a box.\n",
    "\n",
    "## Ingesting and Cleaning Data -\n",
    "First, we took into account the fact we had hundreds of entries from multiple different locations. Due to the fact we had entries from multiple different locations in the county we had a little extra work when it came to cleaning the data. We needed to make sure that each location had its own dataframe, this would make getting specifics on certain locations easier. We achieved this by creating a new dataframe for each location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start lets import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_excel(\"../datasets/Postcard_text_tracker.xlsx\")\n",
    "ds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this dataset some columns are inaccurately names. To make it more readable we have to change this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.rename(columns={\"Unnamed: 2\" : \"Multiple Entries\", \"Unnamed: 3\" : \"Collection Points\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we have to do to clean the data is separate the entries by each location. This entails creating a new dataframe for each location with their specific entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kickoff = df[df[\"Collection Points\"] == \"Kickoff\"]\n",
    "\n",
    "column_text = kickoff.loc[:, 'Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above creates a new dataframe called kickoff. The new dataframe kickoff only contains the entries from the location kickoff. We did this for every location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing and cleaning the data, we began analysis where we had to import a variety of libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first mini project was creating word clouds with the data. To start, for each locations datafame, we turned the text column into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_text\n",
    "\n",
    "text_string = column_text.to_string()\n",
    "\n",
    "text_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the text was turned into a string, we can use this to find all the commonly used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_string\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#if we want to lemmatize\n",
    "#words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "words = [token.text for token in doc if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then when we have all of our common words we can visualize it as a word cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100, mask=None, contour_width=3, contour_color='steelblue').generate(\" \".join(words))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the word cloud above through its width and height, as well as some of its colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things we noticed in our word clouds was that every location seemed to have different common words. While some would be similar, they would never be perfectly the same.\n",
    "\n",
    "Some of the most common words ranged from \"Arlington\", \"Community\", \"Housing\" to \"Trees\", \"Green\", and \"Safe\". \n",
    "\n",
    "All word clouds had their similarities but their differences stuck out more. It was super interesting how one part of arlington focused on community while the other focused on trees and green!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, were going to do sentiment analysis. To start we have to install some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install nbformat\n",
    "pip3 install spacytextblob\n",
    "pip3 install spacy\n",
    "pip3 install textblob\n",
    "pip3 install spacy download en_core_web_sm\n",
    "pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "#Example 1\n",
    "\n",
    "text = text_string\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "doc._.blob.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above test for the polarity of the entire dataframe Kickoff (mentioned earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc._.blob.subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this code above tests the subjectivity of the same dataframe.\n",
    "\n",
    "In the end, we're able to see that while you'd assume that most entries would be positive, not all are. With kickoff having a polarity of 0.2850, it shows us that it is farly positive with some negativity (it ranges from -1 to 1).\n",
    "Though with subjectivity we're able to see that the entries are partially based on opinion. Subjectivity tells us whether something is opinion or not, ranging from 0 to 1 (objective to subjective)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we did was test the semantics of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ./spell_check.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above runs a prewritten spell checker by Mr. Jones on the whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "query = \"missing middle\"\n",
    "def similarityToQuery(text):\n",
    "    return nlp(text).similarity(nlp(query))\n",
    "df_events['similarity_to_query'] = df_events['spell_checked_text'].apply(similarityToQuery)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df_events.sort_values('similarity_to_query', ascending=False).iloc[0][\"concatenated_text\"])\n",
    "print(df_events.sort_values('similarity_to_query', ascending=False).iloc[1][\"concatenated_text\"])\n",
    "print(df_events.sort_values('similarity_to_query', ascending=False).iloc[2][\"concatenated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, I did a Semantic Search within my dataset. This specific code returns 3 comments that mention missing middle or are related to missing middle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - \n",
    "Throughout this project I learned a wide range of topics. This project gave me the chance to solidify my python skills, as well as give me a chance to use them in real world applications. I thought this project was very interesting. It was cool to see the similarities between peoples thoughts and how positive or negative their thoughts were. I think working with survey data is such an interesting topic and should be taught more often. Especially as it gives students a chance to use their skills for real world problems like we did. In the end, this was a very fun project, 10/10 would recomend. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
